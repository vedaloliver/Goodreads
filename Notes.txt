2708

--- have generated new sql table to cater for the year parameter
--- have also contructed a boolean input cascade asking what they want to select

todo 

--- author stats
--- maybe more page stats, against author
--- clean up
--- write readme
--- if they dont want to put onto anki then maybe they should instead write it to a text file???

2508

- a good day... seem to have completely finished off the push to sql file
-- input will ask for a id and it will return and upload all of the data needed as before
-- also will scrape from the website itself and not a html file with a decent amount of speed/efficiency
-- also now pulling the year of publication which is useful

to do:

-- (done) boolean which asks if you want to see your stats, or to push your quotes to an anki package (started on this but gave up cos hungry)
-- loads of stats, look below for cues
-- (gettign ther) figuring out how to run this from bash
-- tidying up
-- figuring out an issue with pushing to git
-- (done)cleaning up the filespace
-- writing a form of readme




2408
- Today i managed to sucessfully change the code so it pulls all of the quotes sucessfully from each file.
-- it did this relatively quickly and i didn't need grequests

Nearly at the end... to finish off you need to:
-set a form of input request which asks the inputter for their id to pull their peronsliased reading list (done)
-- could be possible to only ask for their id, as with it you can sub your id with theirs and keep the format of the list (done)
-add year published (done)
-- will include this in statistical analysis for the average years of the books you have read

-- boolean which asks if you want to see your stats, or to push your quotes to an anki package

- stats
-- get stats for the year of publication
-- potentially stats for year against rating???
-- no of authors with >1 read book
--- average of each author
--- avg page count of each author
-- how many books you read a year
--- what can possibly be done is to see on average how many books you have read a week, asking for a parameter when to start the timeline

1808

formatting notes from soonest at the top cos why tf havent you done this before?
- jesus christ so today you have spend a ludicrous amount of time on pulling the quote links
- first was ordreing greaquetss to come in the correct ordreing
- this ws done, however issues have arose with the fact that it was producing multiple links per result
-- this took at least an hour but was solved by further narrowig down the find functions in the loop
- integration into the main file was a large issue due to an extremely low write speed
-- was circumvented by pulling into a separate function at the top and calling it least
---an issue has arose in the quotes itself in that it is not assigning an id relative to the book its from
-- personally at the moment i can't be fucked to sort this out and maybe indicate that it's not really an issue if i am just 
pulling quotes out, and more the idea that it's not going to be a problem


-- in 35 mins youve managed to locate the area where the quotes are in each web page and pull the quote
--- the data has the quote, the author and the book and you could just user
regular expressions to just separate the data as you cant be bothered to change it
--- this is going to be a priority for tomorrow , it seems realtively easy to go thorugh, hopefully
it isnt slowed as was today
--- also write it to a text file and figuring out how to upload it to anki is another job.






_legacy notes_
what may be a good idea is to utilise a users 'read' pages to scrape a list of the books which have been found
this is possible through a part of the coursera course you completed at an earlier date

you will utliised the goodreads api package to search for the book and get it's url
once within the url, you can set a standard to gather
- page count
- quotes
- average rating
- average year of book read

you will also be able to
- get read dates
- rating averages
- author count

all of this should get exported to sql


0908 - 

managed to quite simply utilise beautiful soup to get the book name and author. to be done for
- read date
-review
-page count
-link
-average rating
-your rating
-would be realtively simple to add it to sql i think
- one issue that stands is that it loads only 30/96 due ot the autoloding nature of the page
- next step add to sql


1308 

- spend a long time on the page incrementer:
first trying to get the autocomplete pag e to work
will do this at a later date - have just got a page which shows 100 of the records as doing it via get delays time
now have all the data onto sql, however an issue arises in that when adding the date read/added field to it, it would not add all 94 entires
have removed that, will come back to it later
-attempted joining author id with title but too tired so will do that later
- generally tidied up the workspace

- for next time:
-- change the database title.title to title.name as its causing issues (done)
-- rewatch the coursera video on join to understand how to link the author and the title (done)
-- attempt to figure out how to add date added and date read while keeping all the entries (done)
-- datetime the two dates (ishy)


1408

-spent a lot of time figuring out sql and have a better understanding of the framework
--have generated join statements which work together
--the whole table was not generating as date_added needed to be in it's own independent table as duplicate date adds were only
--causing one of that dates books to be uploaded. have made a new table for date added to avoid this 

-the data can not be parsed with datetime module at this current time but will be done when pulling back into another python file
-- but looking at below, it could be possible

--just managed to amend isbn number to produce null if no number is provided by a rough if statement if there is no digit below 6. a better way
of doing it for sure but this works

-- pushed to git for the first time, a little bit rusty on this but try to remember how to do it again

- for next time:
-- see if you can do a statement which parses date into datetime format (done)
-- amend the date_read data so there actually is data 
-- create a table for date_read a la date_added
-- create another .py  (done)
--- will have quote pulling element which writes to a simple .txt for the moment
---- can figure out how this pushes to anki in the future
--- a data analyser, using matplotlib to visualise this 

1508

- got an new py file
idea can be to generate a lot of functions for the different reasons:
- data analysis

-- page count : 
--- avg page of a book (done)
--- std deviation (done)
--- median (dont really need)
--- page count ranking by author 

-- proportion of books read by the same author
--- also proportion of authors youve read more than one book with

-- date added over time

-- avg rating
--- avg rating of the author combining all their books
---avg rating over time


1608

- got a better understanding of the debugger functions
- laid down various elements and figured out pulling data from the sql databases

to do next time is just copying from above:
author:

_____stats_____
--- page count ranking by author 
-- proportion of books read by the same author

-- avg rating
--- avg rating of the author combining all their books
---avg rating of the stuff you've read over time

-- quote generator _____
--- took you better part of an hour to acsess the webpage of the book itself and fuck around with the html stuff
to acsess the link for the quote page
--- what may be a good idea is to integrate this into the first document - while it may seem a little complex
its probably not that hard
--- will involve looping through each link at a time, obtaining the quote link
--- youve done this before on the coursera link


1